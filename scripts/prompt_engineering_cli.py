#!/usr/bin/env python3
"""
Prompt Engineering CLI - –ì–ª–∞–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–æ–º–ø—Ç-–∏–Ω–∂–µ–Ω–µ—Ä–∞

–ö–æ–º–∞–Ω–¥—ã:
  metrics     - –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–º–ø—Ç–∞
  ab-test     - –ó–∞–ø—É—Å—Ç–∏—Ç—å A/B —Ç–µ—Å—Ç –º–µ–∂–¥—É –ø—Ä–æ–º–ø—Ç–∞–º–∏
  prompt      - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏—è–º–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
  benchmark   - –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  report      - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–æ–≤
  judge       - LLM-as-a-Judge –æ—Ü–µ–Ω–∫–∞
  augment     - –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
  optimize    - –ê–≤—Ç–æ-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
  plot        - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤

–ü—Ä–∏–º–µ—Ä—ã:
  ./prompt_engineering_cli.py metrics --test-results results.json
  ./prompt_engineering_cli.py plot --type history --data history.json
"""

import sys
import argparse
import json
from pathlib import Path

# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
sys.path.insert(0, str(Path(__file__).parent.parent))

from prompt_engineering.metrics_calculator import MetricsCalculator
from prompt_engineering.ab_testing import ABTester, PromptVariant
from prompt_engineering.prompt_manager import PromptManager
from prompt_engineering.advanced_tools import LLMJudge, DatasetAugmenter, PromptOptimizer
from prompt_engineering.visualization import Visualizer


class PromptEngineeringCLI:
    """CLI –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏"""

    def __init__(self):
        self.metrics_calc = MetricsCalculator()
        self.ab_tester = ABTester()
        self.prompt_manager = PromptManager()
        self.judge = LLMJudge()
        self.augmenter = DatasetAugmenter()
        self.optimizer = PromptOptimizer()
        self.visualizer = Visualizer()

    # === PLOT ===
    def cmd_plot(self, args):
        """–ö–æ–º–∞–Ω–¥–∞: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è"""
        print(f"\n{'='*80}")
        print("üìà –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–†–ê–§–ò–ö–û–í")
        print(f"{'='*80}")
        
        if not args.data:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ --data (JSON —Ñ–∞–π–ª)")
            return

        with open(args.data, 'r') as f:
            data = json.load(f)

        if args.type == 'history':
            path = self.visualizer.plot_version_history(data)
            print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")
            
        elif args.type == 'confusion':
            path = self.visualizer.plot_confusion_matrix(
                data['y_true'], data['y_pred'], data['labels']
            )
            print(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")
            
        elif args.type == 'judge':
            path = self.visualizer.plot_judge_distribution(data)
            print(f"‚úÖ –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")

    # === JUDGE ===
    def cmd_judge(self, args):
        """–ö–æ–º–∞–Ω–¥–∞: LLM-as-a-Judge"""
        print(f"\n{'='*80}")
        print("‚öñÔ∏è LLM –°–£–î–¨–Ø")
        print(f"{'='*80}")
        
        result = self.judge.evaluate(
            question=args.question,
            answer=args.answer,
            ground_truth=args.ground_truth,
            criteria=args.criteria or "accuracy"
        )
        
        print(f"\nüèÜ –û—Ü–µ–Ω–∫–∞: {result.score}/5")
        print(f"üìù –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {result.reasoning}")

    # === AUGMENT ===
    def cmd_augment(self, args):
        """–ö–æ–º–∞–Ω–¥–∞: –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        print(f"\n{'='*80}")
        print("üß¨ –ì–ï–ù–ï–†–ê–¶–ò–Ø –í–ê–†–ò–ê–¶–ò–ô (Augmentation)")
        print(f"{'='*80}")
        
        examples = []
        if args.input:
            with open(args.input, 'r') as f:
                examples = json.load(f)
        elif args.text:
            examples = [args.text]
            
        print(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print("–ì–µ–Ω–µ—Ä–∏—Ä—É—é...")
        
        variations = self.augmenter.augment(examples, n_variations=args.n)
        
        print(f"\n‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(variations)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:\n")
        for v in variations:
            print(f"  - {v}")
            
        if args.output:
            with open(args.output, 'w') as f:
                json.dump(variations, f, ensure_ascii=False, indent=2)
            print(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {args.output}")

    # === OPTIMIZE ===
    def cmd_optimize(self, args):
        """–ö–æ–º–∞–Ω–¥–∞: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞"""
        print(f"\n{'='*80}")
        print("‚ú® –ê–í–¢–û-–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–†–û–ú–ü–¢–ê")
        print(f"{'='*80}")
        
        # –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–º–ø—Ç
        try:
            current_prompt = self.prompt_manager.get_prompt(args.prompt_name).prompt_text
        except:
            print(f"‚ùå –ü—Ä–æ–º–ø—Ç '{args.prompt_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ—à–∏–±–∫–∏
        with open(args.failures, 'r') as f:
            failures = json.load(f)
            
        print(f"–ê–Ω–∞–ª–∏–∑ {len(failures)} –æ—à–∏–±–æ–∫...")
        optimized_text = self.optimizer.optimize(current_prompt, failures)
        
        print("\nüí° –ü–†–ï–î–õ–û–ñ–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢:\n")
        print(optimized_text)
        
        if input("\n–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é? (y/n): ").lower() == 'y':
            self.prompt_manager.update_prompt(
                prompt_name=args.prompt_name,
                prompt_text=optimized_text,
                description="Auto-optimized",
                author="OptimizerAI"
            )
            print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")

    def cmd_metrics(self, args):
        """–ö–æ–º–∞–Ω–¥–∞: —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫"""
        print(f"\n{'='*80}")
        print("üìä –†–ê–°–ß–Å–¢ –ú–ï–¢–†–ò–ö")
        print(f"{'='*80}")

        if args.test_results:
            with open(args.test_results, 'r', encoding='utf-8') as f:
                results = json.load(f)

            print(f"\n–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {len(results.get('results', []))}")

            if args.metrics_type == "classification":
                tp = args.true_positives or 0
                fp = args.false_positives or 0
                fn = args.false_negatives or 0
                tn = args.true_negatives or 0

                result = self.metrics_calc.calculate_classification_metrics(tp, fp, fn, tn)

                print(f"\n‚úÖ Classification Metrics:")
                print(f"  Precision: {result.precision:.3f}")
                print(f"  Recall: {result.recall:.3f}")
                print(f"  F1 Score: {result.f1_score:.3f}")
                print(f"  Accuracy: {result.accuracy:.3f}")
                print(f"  Support: {result.support}")

        print(f"\n‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã!")

    def cmd_ab_test(self, args):
        """–ö–æ–º–∞–Ω–¥–∞: A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        print(f"\n{'='*80}")
        print("üß™ A/B –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
        print(f"{'='*80}")

        try:
            prompt_a = self.prompt_manager.get_prompt(args.variant_a)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –≤–∞—Ä–∏–∞–Ω—Ç A: {args.variant_a} (v{prompt_a.version})")
        except:
            print(f"‚ùå –ü—Ä–æ–º–ø—Ç '{args.variant_a}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        try:
            prompt_b = self.prompt_manager.get_prompt(args.variant_b)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –≤–∞—Ä–∏–∞–Ω—Ç B: {args.variant_b} (v{prompt_b.version})")
        except:
            print(f"‚ùå –ü—Ä–æ–º–ø—Ç '{args.variant_b}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        if args.test_data:
            with open(args.test_data, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(test_data)}")
            print("\n‚ö†Ô∏è A/B —Ç–µ—Å—Ç —Ç—Ä–µ–±—É–µ—Ç executor_func –∏ metrics_func")
            print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Python API –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞")
        else:
            print("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (--test-data)")

    def cmd_prompt(self, args):
        """–ö–æ–º–∞–Ω–¥–∞: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞–º–∏"""
        print(f"\n{'='*80}")
        print("üìù –£–ü–†–ê–í–õ–ï–ù–ò–ï –ü–†–û–ú–ü–¢–ê–ú–ò")
        print(f"{'='*80}")

        if args.action == "list":
            prompts = self.prompt_manager.list_prompts()
            if not prompts:
                print("\n–ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤")
            else:
                print(f"\n–í—Å–µ–≥–æ –ø—Ä–æ–º–ø—Ç–æ–≤: {len(prompts)}\n")
                for p in prompts:
                    print(f"  üìÑ {p['name']}")
                    print(f"     –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è: v{p['current_version']}")
                    print(f"     –í—Å–µ–≥–æ –≤–µ—Ä—Å–∏–π: {p['total_versions']}")
                    print(f"     –°–æ–∑–¥–∞–Ω: {p['created_at'][:10]}")
                    print()

        elif args.action == "create":
            if not args.name or not args.text:
                print("‚ùå –£–∫–∞–∂–∏—Ç–µ --name –∏ --text")
                return
            prompt = self.prompt_manager.create_prompt(
                prompt_name=args.name,
                prompt_text=args.text,
                description=args.description or "No description",
                author=args.author or "prompt_engineer"
            )
            print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω –ø—Ä–æ–º–ø—Ç '{args.name}' –≤–µ—Ä—Å–∏—è {prompt.version}")

        elif args.action == "update":
            if not args.name or not args.text:
                print("‚ùå –£–∫–∞–∂–∏—Ç–µ --name –∏ --text")
                return
            prompt = self.prompt_manager.update_prompt(
                prompt_name=args.name,
                prompt_text=args.text,
                description=args.description or "Update",
                version_type=args.version_type or "minor",
                author=args.author or "prompt_engineer"
            )
            print(f"\n‚úÖ –û–±–Ω–æ–≤–ª—ë–Ω –ø—Ä–æ–º–ø—Ç '{args.name}' –≤–µ—Ä—Å–∏—è {prompt.version}")

        elif args.action == "show":
            if not args.name:
                print("‚ùå –£–∫–∞–∂–∏—Ç–µ --name")
                return
            prompt = self.prompt_manager.get_prompt(args.name, args.version)
            print(f"\nüìÑ –ü—Ä–æ–º–ø—Ç: {args.name}")
            print(f"–í–µ—Ä—Å–∏—è: v{prompt.version}")
            print(f"–ê–≤—Ç–æ—Ä: {prompt.author}")
            print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {prompt.description}")
            print(f"\n–¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞:\n{'-'*80}\n{prompt.prompt_text}\n{'-'*80}")

        elif args.action == "versions":
            if not args.name:
                print("‚ùå –£–∫–∞–∂–∏—Ç–µ --name")
                return
            versions = self.prompt_manager.list_versions(args.name)
            print(f"\n–í–µ—Ä—Å–∏–∏ –ø—Ä–æ–º–ø—Ç–∞ '{args.name}':\n")
            for v in versions:
                print(f"  ‚Ä¢ v{v}")

        elif args.action == "compare":
            if not args.name or not args.version or not args.version2:
                print("‚ùå –£–∫–∞–∂–∏—Ç–µ --name, --version –∏ --version2")
                return
            comparison = self.prompt_manager.compare_versions(args.name, args.version, args.version2)
            print(f"\nüîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π {args.version} –∏ {args.version2}:")
            print(f"–°—Ö–æ–∂–µ—Å—Ç—å: {comparison['similarity']*100:.1f}%")
            print(f"–î–ª–∏–Ω–∞: {comparison['length_a']} ‚Üí {comparison['length_b']}")
            print(f"\nDiff:")
            for line in comparison['diff']:
                print(f"  {line}")

        elif args.action == "export":
            if not args.name:
                print("‚ùå –£–∫–∞–∂–∏—Ç–µ --name")
                return
            export_data = self.prompt_manager.export_prompt(args.name, args.version)
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    f.write(export_data)
                print(f"‚úÖ –ü—Ä–æ–º–ø—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {args.output}")
            else:
                print(export_data)

    def cmd_benchmark(self, args):
        print(f"\n{'='*80}\n‚ö° BENCHMARK\n{'='*80}")
        print("\n‚ö†Ô∏è Benchmark —Ç—Ä–µ–±—É–µ—Ç integration —Å Vision API")

    def cmd_report(self, args):
        print(f"\n{'='*80}\nüìä –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–Å–¢–û–í\n{'='*80}")
        if args.experiment_dir:
            print(f"\n–ê–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤ {args.experiment_dir}")
            exp_dir = Path(args.experiment_dir)
            if not exp_dir.exists():
                print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.experiment_dir}")
                return
            reports = list(exp_dir.glob("**/report.json"))
            print(f"–ù–∞–π–¥–µ–Ω–æ –æ—Ç—á—ë—Ç–æ–≤: {len(reports)}\n")
            for report_file in reports:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report = json.load(f)
                print(f"  üìÑ {report['test_name']}")
                print(f"     –ü–æ–±–µ–¥–∏—Ç–µ–ª—å: {report['winner']}")
                print(f"     –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {report['confidence']*100:.1f}%")
                print()
        else:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ --experiment-dir")


def main():
    parser = argparse.ArgumentParser(description="Prompt Engineering CLI", formatter_class=argparse.RawDescriptionHelpFormatter)
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥—ã')

    # === METRICS ===
    metrics_parser = subparsers.add_parser('metrics', help='–†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫')
    metrics_parser.add_argument('--test-results', help='JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã')
    metrics_parser.add_argument('--metrics-type', choices=['classification'], default='classification')
    metrics_parser.add_argument('--true-positives', type=int)
    metrics_parser.add_argument('--false-positives', type=int)
    metrics_parser.add_argument('--false-negatives', type=int)
    metrics_parser.add_argument('--true-negatives', type=int)

    # === A/B TEST ===
    ab_parser = subparsers.add_parser('ab-test', help='A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ')
    ab_parser.add_argument('--variant-a', required=True)
    ab_parser.add_argument('--variant-b', required=True)
    ab_parser.add_argument('--test-data')
    ab_parser.add_argument('--sample-size', type=int)

    # === PROMPT ===
    prompt_parser = subparsers.add_parser('prompt', help='–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞–º–∏')
    prompt_parser.add_argument('action', choices=['list', 'create', 'update', 'show', 'versions', 'compare', 'export'])
    prompt_parser.add_argument('--name')
    prompt_parser.add_argument('--text')
    prompt_parser.add_argument('--description')
    prompt_parser.add_argument('--author')
    prompt_parser.add_argument('--version')
    prompt_parser.add_argument('--version2')
    prompt_parser.add_argument('--version-type', choices=['major', 'minor', 'patch'])
    prompt_parser.add_argument('--output')

    # === BENCHMARK ===
    benchmark_parser = subparsers.add_parser('benchmark', help='–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–º–ø—Ç–∞')
    benchmark_parser.add_argument('--prompt', required=True)
    benchmark_parser.add_argument('--dataset', required=True)
    benchmark_parser.add_argument('--output')

    # === REPORT ===
    report_parser = subparsers.add_parser('report', help='–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–æ–≤')
    report_parser.add_argument('--experiment-dir')

    # === JUDGE ===
    judge_parser = subparsers.add_parser('judge', help='–û—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM')
    judge_parser.add_argument('--question', required=True)
    judge_parser.add_argument('--answer', required=True)
    judge_parser.add_argument('--ground-truth')
    judge_parser.add_argument('--criteria')

    # === AUGMENT ===
    augment_parser = subparsers.add_parser('augment', help='–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö')
    augment_parser.add_argument('--text')
    augment_parser.add_argument('--input')
    augment_parser.add_argument('--output')
    augment_parser.add_argument('--n', type=int, default=3)

    # === OPTIMIZE ===
    opt_parser = subparsers.add_parser('optimize', help='–ê–≤—Ç–æ-—É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞')
    opt_parser.add_argument('--prompt-name', required=True)
    opt_parser.add_argument('--failures', required=True)

    # === PLOT ===
    plot_parser = subparsers.add_parser('plot', help='–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤')
    plot_parser.add_argument('--type', choices=['history', 'confusion', 'judge'], required=True)
    plot_parser.add_argument('--data', required=True)

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    cli = PromptEngineeringCLI()

    if args.command == 'metrics': cli.cmd_metrics(args)
    elif args.command == 'ab-test': cli.cmd_ab_test(args)
    elif args.command == 'prompt': cli.cmd_prompt(args)
    elif args.command == 'benchmark': cli.cmd_benchmark(args)
    elif args.command == 'report': cli.cmd_report(args)
    elif args.command == 'judge': cli.cmd_judge(args)
    elif args.command == 'augment': cli.cmd_augment(args)
    elif args.command == 'optimize': cli.cmd_optimize(args)
    elif args.command == 'plot': cli.cmd_plot(args)

if __name__ == "__main__":
    main()